{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c184178b-bbe5-46bc-9b5b-43468633faf4",
   "metadata": {},
   "source": [
    "#### Nama  : Dohan Rizqi Hadityo\n",
    "\n",
    "#### NIM   : 210411100195\n",
    "\n",
    "#### Kelas : Pencarian dan Penambangan Web A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbd99b-2a74-4421-8528-7f67db9b647e",
   "metadata": {},
   "source": [
    "# Pengertian Crawling Data\r\n",
    "\r\n",
    "Crawling data, atau web crawling, adalah proses otomatis untuk menelusuri dan mengumpulkan data dari berbagai halaman web di internet. Proses ini dilakukan oleh program atau bot yang disebut \"web crawler\" atau \"spider\".\r\n",
    "\r\n",
    "## Tujuan Crawling Data\r\n",
    "\r\n",
    "Crawling data digunakan untuk berbagai tujuan, termasuk:\r\n",
    "\r\n",
    "- **Mengindeks Konten Web**: Mesin pencari seperti Google menggunakan web crawler untuk mengindeks konten dari seluruh internet, sehingga pengguna dapat menemukan informasi yang relevan dengan lebih mudah.\r\n",
    "\r\n",
    "- **Mengumpulkan Data untuk Analisis**: Perusahaan atau peneliti dapat menggunakan crawling data untuk mengumpulkan informasi dari web untuk keperluan analisis bisnis, penelitian pasar, atau penelitian akademis.\r\n",
    "\r\n",
    "- **Memantau Perubahan**: Beberapa organisasi menggunakan web crawler untuk memantau perubahan pada situs web tertentu, seperti perubahan harga, konten berita, atau update produk.\r\n",
    "pdate produk.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b663d-088a-4146-aba8-ffbd9347a3cd",
   "metadata": {},
   "source": [
    "## Cara Kerja Crawling Data\n",
    "\n",
    "\n",
    "1. **Memulai dengan URL Awal (Seed URL)**\r\n",
    "   \r\n",
    "   Proses crawling data dimulai dengan satu atau lebih URL awal yang disebut \"seed URL\". URL ini bertindak sebagai titik awal bagi web crawler untuk mulai menjelajahi web. Seed URL biasanya dipilih berdasarkan prioritas atau relevansi, misalnya, halaman web yang memiliki banyak tautan keluar atau sering diperbarui.\r\n",
    "\r\n",
    "2. **Mengunduh Halaman Web**\r\n",
    "   \r\n",
    "   Setelah menentukan seed URL, web crawler mengunjungi halaman web tersebut dan mengunduh seluruh kontennya. Konten ini bisa berupa teks, gambar, video, serta tautan ke halaman web lainnya. Proses ini sering kali melibatkan permintaan HTTP ke server web yang meng-host halaman tersebut.\r\n",
    "\r\n",
    "3. **Mengekstraksi Tautan (Link Extraction)**\r\n",
    "   \r\n",
    "   Setelah halaman web diunduh, crawler memindai konten untuk menemukan semua tautan (hyperlink) yang terdapat di dalamnya. Tautan ini kemudian ditambahkan ke dalam daftar URL yang akan dikunjungi selanjutnya. Proses ini memungkinkan crawler untuk berpindah dari satu halaman ke halaman lainnya, secara efektif menjelajahi web.\r\n",
    "\r\n",
    "4. **Mengikuti Tautan dan Mengulangi Proses**\r\n",
    "   \r\n",
    "   Web crawler kemudian mengikuti tautan yang diekstraksi dan mengunjungi halaman-halaman tersebut. Setelah tiba di halaman baru, crawler akan mengulangi proses pengunduhan, pengekstrakan tautan, dan mengikuti tautan. Proses ini terus berlangsung secara rekursif, memungkinkan crawler untuk menjelajahi berbagai bagian web.\r\n",
    "\r\n",
    "5. **Menghindari Duplikasi dan Mengelola Antrian URL**\r\n",
    "   \r\n",
    "   Untuk menghindari mengunjungi halaman yang sama berkali-kali, crawler menggunakan struktur data seperti hash table atau set untuk menyimpan URL yang sudah dikunjungi. Crawler juga sering kali menggunakan antrian (queue) untuk mengatur urutan URL yang akan dikunjungi berdasarkan prioritas atau strategi crawling (misalnya, breadth-first search atau depth-first search).\r\n",
    "\r\n",
    "6. **Menghormati Protokol `robots.txt`**\r\n",
    "   \r\n",
    "   Sebelum mengunjungi halaman, crawler biasanya memeriksa file `robots.txt` yang ada di root domain situs web. File ini memberikan instruksi tentang bagian mana dari situs yang boleh atau tidak boleh diakses oleh crawler. Dengan menghormati instruksi ini, crawler membantu menjaga etika dan kepatuhan terhadap kebijakan privasi situs web.\r\n",
    "\r\n",
    "7. **Penyimpanan dan Pengolahan Data**\r\n",
    "   \r\n",
    "   Setelah data diunduh, informasi tersebut disimpan dalam database atau sistem penyimpanan lainnya untuk dianalisis atau diindeks lebih lanjut. Data ini bisa digunakan oleh mesin pencari untuk membuat indeks pencarian, atau oleh perusahaan untuk analisis bisnis dan riset pasar.\r\n",
    "\r\n",
    "8. **Pengelolaan Sumber Daya**\r\n",
    "   \r\n",
    "   Crawling data adalah proses yang membutuhkan banyak sumber daya, termasuk bandwidth internet dan kapasitas penyimpanan. Oleh karena itu, crawler perlu dirancang untuk mengelola penggunaan sumber daya ini dengan efisien, termasuk mengatur kecepatan crawling agar tidak membebani server web yang dikunjungi.\r\n",
    "\r\n",
    "9. **Menghadapi Batasan dan Rintangan**\r\n",
    "   \r\n",
    "   Crawler juga harus mampu menangani berbagai rintangan teknis, seperti halaman web yang memerlukan autentikasi, konten dinamis yang dihasilkan oleh JavaScript, atau batasan dari server yang membatasi jumlah permintaan dalam periode waktu tertentu.\r\n",
    "\r\n",
    "10. **Penghentian atau Penyelesaian Crawling**\r\n",
    "   \r\n",
    "   Crawling dapat dihentikan berdasarkan beberapa kondisi, seperti:\r\n",
    "\r\n",
    "   - Jumlah halaman yang dikunjungi telah mencapai batas yang ditentukan.\r\n",
    "   - Waktu yang dialokasikan untuk crawling telah habis.\r\n",
    "   - Semua tautan yang ditemukan telah dikunjungi.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdb873-c73b-4d6a-9a53-267f4e1e3bdb",
   "metadata": {},
   "source": [
    "## Beberapa Library yang Digunakan untuk Crawling Data\n",
    "\n",
    "### 1. Requests\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "Requests adalah library Python yang digunakan untuk mengirim permintaan HTTP/HTTPS ke server web dan mengunduh konten halaman. Library ini memudahkan proses mengakses halaman web dengan menyediakan antarmuka yang sederhana untuk mengirim permintaan GET atau POST, menangani cookies, dan mengelola session.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Mengunduh halaman web dan sumber daya lainnya dari internet.\r\n",
    "- Mengirim data ke server melalui permintaan POST.\r\n",
    "- Menangani autentikasi, redirect, dan cookies.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. BeautifulSoup\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "BeautifulSoup adalah library Python yang digunakan untuk parsing HTML dan XML. Library ini memungkinkan pengguna untuk menavigasi, mencari, dan memodifikasi pohon parse (parse tree) dari halaman web. BeautifulSoup sangat berguna untuk mengekstraksi informasi tertentu dari HTML, seperti tautan, teks, atau tag tertentu.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Mem-parsing dan mengekstraksi data dari HTML atau XML.\r\n",
    "- Navigasi dan pencarian elemen dalam dokumen HTML menggunakan sintaks yang mirip dengan CSS selector.\r\n",
    "- Memodifikasi dokumen HTML.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Scrapy\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "Scrapy adalah framework crawling dan scraping data yang kuat dan fleksibel untuk Python. Scrapy dirancang untuk mengotomatisasi ekstraksi data dari situs web dengan kecepatan tinggi dan efisiensi. Framework ini menyediakan berbagai fitur seperti penanganan URL, manajemen cookies, dan pengaturan throttling untuk mengontrol kecepatan crawling.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Meng-crawl situs web dan mengumpulkan data dengan cepat dan efisien.\r\n",
    "- Mendukung ekstraksi data terstruktur dan semi-terstruktur dari halaman web.\r\n",
    "- Memiliki dukungan bawaan untuk menangani banyak hal seperti cookies, redirect, dan user-agent.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Selenium\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "Selenium adalah library Python yang digunakan untuk otomatisasi browser. Selenium sangat berguna untuk scraping data dari halaman web yang menggunakan JavaScript secara dinamis untuk memuat kontennya. Selenium memungkinkan kita untuk mengontrol browser secara langsung, melakukan klik, input teks, dan interaksi lain seperti manusia.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Mengotomatisasi browser untuk berinteraksi dengan halaman web.\r\n",
    "- Menangani situs web dengan konten dinamis yang dimuat melalui JavaScript.\r\n",
    "- Mengambil screenshot atau mengunduh sumber daya dari situs web.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Puppeteer\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "Puppeteer adalah library untuk Node.js yang menyediakan API tingkat tinggi untuk mengontrol Chrome atau Chromium melalui Protokol DevTools. Puppeteer sering digunakan untuk mengotomatisasi pengambilan konten dari halaman web dengan JavaScript yang berat, karena bisa melakukan rendering lengkap dari halaman seperti browser manusia.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Menyediakan kemampuan untuk mengotomatisasi browser tanpa kepala (headless browser automation).\r\n",
    "- Mengambil screenshot, mengunduh PDF, dan mengekstraksi konten dari halaman web dengan rendering lengkap.\r\n",
    "- Mendukung scraping data dari halaman web yang kompleks dengan JavaScript yang berat.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. lxml\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "lxml adalah library Python untuk parsing XML dan HTML dengan cepat dan efisien. Library ini menggabungkan kecepatan parsing XML C libraries (libxml2 dan libxslt) dengan API yang mudah digunakan dan kaya fitur.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Parsing dan memanipulasi dokumen XML dan HTML dengan cepat.\r\n",
    "- Menyediakan dukungan XPath dan XSLT untuk navigasi dan transformasi dokumen.\r\n",
    "- Memiliki kinerja yang sangat baik untuk memproses dokumen besar.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 7. Pyppeteer\r\n",
    "\r\n",
    "**Pengertian:**  \r\n",
    "Pyppeteer adalah porting Puppeteer ke Python. Ini memungkinkan Anda untuk menggunakan API Puppeteer yang sama untuk mengotomatisasi Chrome/Chromium melalui Python, mendukung headless browsing, dan pengambilan data dari halaman web dengan JavaScript yang berat.\r\n",
    "\r\n",
    "**Kegunaan:**\r\n",
    "\r\n",
    "- Mengotomatisasi browser Chrome/Chromium dari Python.\r\n",
    "- Mendukung scraping data dari halaman web yang menggunakan JavaScript untuk memuat konten.\r\n",
    "- Mengambil screenshot, membuat PDF, dan mengekstrak data dari halaman web.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1e9a04-a53a-40fb-9b01-d22cc8f484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dohan\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\dohan\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dohan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625d5e8c-ef79-40dd-baac-163fb122de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921e7f96-a525-45e4-83fe-1cd9ff3dfe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No                                       Judul Berita  \\\n",
      "0   1  Cabuli Anak 14 Tahun saat Tertidur, Pria di Ba...   \n",
      "1   2  Imam Besar Masjid Istiqlal Minta Paus Fransisk...   \n",
      "2   3  Per Agustus 2024, Populasi Motor Listrik Dikla...   \n",
      "3   4           7 Contoh Pajak yang Termasuk Pajak Pusat   \n",
      "4   5  Profil Eka Maulana, Pengusaha Muda yang Maju d...   \n",
      "\n",
      "                                          Isi Berita Tanggal Berita  \\\n",
      "0  BULELENG, KOMPAS.com - Seorang pria berinisial...     05/09/2024   \n",
      "1  JAKARTA, KOMPAS.com - Imam Besar Masjid Istiql...     05/09/2024   \n",
      "2  JAKARTA, KOMPAS.com - Ketua Umum Asosiasi Indu...     05/09/2024   \n",
      "3  KOMPAS.com - Pajak adalah kontribusi wajib yan...     05/09/2024   \n",
      "4  BOGOR, KOMPAS.com- Eka Maulana ikut berkontest...     05/09/2024   \n",
      "\n",
      "  Kategori Berita  \n",
      "0        REGIONAL  \n",
      "1            NEWS  \n",
      "2        OTOMOTIF  \n",
      "3           MONEY  \n",
      "4            NEWS  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL halaman indeks berita\n",
    "base_url = \"https://indeks.kompas.com/?page=\"\n",
    "\n",
    "# Menyimpan hasil scraping\n",
    "data_berita = []\n",
    "\n",
    "# Mengambil artikel dari beberapa halaman\n",
    "for page in range(1, 3):  # Mengambil halaman 1 dan 2\n",
    "    indeks_url = f\"{base_url}{page}\"\n",
    "    page_response = requests.get(indeks_url)\n",
    "    \n",
    "    # Cek apakah request sukses atau gagal\n",
    "    if page_response.status_code == 200:\n",
    "        soup_page = BeautifulSoup(page_response.text, 'html.parser')\n",
    "        artikel_items = soup_page.find_all('div', class_='articleItem')\n",
    "\n",
    "        # Cek apakah ada artikel di halaman tersebut\n",
    "        if artikel_items:\n",
    "            # Looping setiap artikel\n",
    "            for artikel in artikel_items:\n",
    "                link_artikel = artikel.find('a', class_='article-link')\n",
    "                judul_artikel = artikel.find('h2', class_='articleTitle')\n",
    "                kategori_artikel = artikel.find('div', class_='articlePost-subtitle')\n",
    "                tanggal_artikel = artikel.find('div', class_='articlePost-date')\n",
    "\n",
    "                # Cek apakah semua elemen artikel ditemukan\n",
    "                if link_artikel and judul_artikel and kategori_artikel and tanggal_artikel:\n",
    "                    artikel_link = link_artikel['href']\n",
    "                    judul_artikel = judul_artikel.get_text().strip()\n",
    "                    kategori_artikel = kategori_artikel.get_text().strip()\n",
    "                    tanggal_artikel = tanggal_artikel.get_text().strip()\n",
    "\n",
    "                    # Mendapatkan isi artikel dari halaman artikel\n",
    "                    artikel_response = requests.get(artikel_link)\n",
    "                    \n",
    "                    # Cek apakah request sukses\n",
    "                    if artikel_response.status_code == 200:\n",
    "                        artikel_soup = BeautifulSoup(artikel_response.text, 'html.parser')\n",
    "                        paragraf_isi = artikel_soup.find('div', class_='read__content').find_all('p')\n",
    "                        \n",
    "                        # Cek apakah konten ditemukan\n",
    "                        if paragraf_isi:\n",
    "                            isi_artikel = ' '.join([p.get_text().strip() for p in paragraf_isi])\n",
    "                        else:\n",
    "                            isi_artikel = 'Konten tidak tersedia'\n",
    "\n",
    "                        # Menyimpan data ke dalam list\n",
    "                        data_berita.append([judul_artikel, isi_artikel, tanggal_artikel, kategori_artikel])\n",
    "                    else:\n",
    "                        print(f\"Error saat mengambil artikel: {artikel_link}\")\n",
    "                else:\n",
    "                    print(f\"Artikel tidak lengkap di halaman {page}\")\n",
    "        else:\n",
    "            print(f\"Tidak ada artikel di halaman {page}\")\n",
    "    else:\n",
    "        print(f\"Error mengambil halaman {page}\")\n",
    "\n",
    "# Mengonversi data ke dalam DataFrame\n",
    "df_berita = pd.DataFrame(data_berita, columns=['Judul Berita', 'Isi Berita', 'Tanggal Berita', 'Kategori Berita'])\n",
    "\n",
    "# Menambahkan kolom penomoran\n",
    "df_berita.insert(0, 'No', range(1, 1 + len(df_berita)))\n",
    "\n",
    "# Menampilkan DataFrame\n",
    "print(df_berita.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d53e33d9-66c2-4f99-a479-9a600a8f44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berita telah disimpan ke berita_kompas_terbaru.csv\n"
     ]
    }
   ],
   "source": [
    "csv_filename = \"berita_kompas_terbaru.csv\"\n",
    "df_berita.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data berita telah disimpan ke {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d290ec8-88d3-43c1-b1de-2c115e69f820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
